---
title: "Analysis of Systems’ Performance in Competitions Challenges"
author:
  - name: Sergio M. Nava Muñoz
    id: sn
    email: nava@cimat.mx
    affiliation: 
        - id: cimat
          name: CIMAT
          city: Aguascalientes
          state: Ags
        - name: INFOTEC
          city: Aguascalientes
          state: Ags
  - name: PhD Mario Graff Guerrero
    id: mgg
    degrees: PhD
    email: mario.graff@infotec.edu.mx
    affiliation: 
      - name: INFOTEC
        city: Aguascalientes
        state: Ags
institute:  "Doctorado en Ciencias en Ciencia de Datos"
date: 2024-11-27
date-format: long
format: 
  revealjs:
    slide-number: true
    theme: default
    fontsize: 1.5em
    logo: figs/logo-infotec.jpeg
    css: style.css
    chalkboard: true
    menu: TRUE
    transition: slide
    background-transition: fade
    title-slide-attributes:
      data-background-image: /figs/DCCD1.png
      data-background-size: contain 
toc: TRUE
toc-depth: 2
---


# Introduction

---

## Challenge 



![Challenge](figs/scheme.png){#fig-Challenge width="600" height="250"}



:::: {.fragment }

###  Restrictions

- Comparison of multiple participants (systems)
- Selected performance metrics
- A test dataset of fixed size \( $n$ \)
- A limited number of submissions per participant
- The Reference (Gold Standard) is known just for the organizers

::::


## Results for VaxxStance Close track - contextual {.smaller}


:::: {.fragment }

:::  custom-small
| Obs  | Gold Standard | WordUp.01 | ...   | SQYQP.01 |
|------|---------------|-----------|-------|----------|
| 1    | favor         | favor     | ...   | favor    |
| 2    | favor         | favor     | ...   | none     |
| 3    | against       | none      | ...   | against  |
| 4    | none          | none      | ...   | none     |
| ...  | ...           | ...       | ...   | ...      |
| $n_{test}$ | none    | favor     | ...   | against  |

:  Results for VaxxStance Close track - contextual {#tbl-VaxxStance1 .striped .hover }

:::

::::

:::: {.fragment .fade-in-then-out}

### Leaderboard

::: custom-small  
| System              | Basque  |
|---------------------|---------|
| WordUp.01           | 0.5734  |
| WordUp.02           | 0.5465  |
| MultiAztertest.01   | 0.5024  |
| SQYQP.01            | 0.4256  |
| MultiAztertest.02   | 0.3428  |

:  Results using Macro-averaged F1 Score for "favor" and "against". {#tbl-VaxxStance2 .striped .hover}
:::

::::

## Objective

### General objective

To investigate and implement an evaluation scheme based on robust statistical methods for learning algorithms in the context of challenges, providing organizers and researchers with concrete and validated tools for more precise and well-founded decision-making.

### Specific objectives



1. **Thoroughly review the different elements used in evaluating learning algorithms**, including performance metrics, statistical methods used to infer significance, and test configurations. This review will focus on identifying the most common practices and their limitations to establish a theoretical framework that supports the design of new evaluation schemes.
2. **Examine in detail the characteristics and criteria used by the main Machine Learning competitions**, defining as main those with a high impact on the scientific community or pioneers in applying new evaluation methodologies. This objective will include an analysis of result validation processes, the transparency and fairness of evaluations, and the methodologies used for data handling and distribution.
3. **Design evaluation schemes for learning algorithms that integrate advanced statistical tools** based on the deficiencies identified in the previous objectives. This design will include prototype schemes evaluated through simulations or collaboration with existing competition organizers to validate their effectiveness and practicality. Clear guidelines for their implementation in different competition contexts will also be developed.



# Proposed methodology

## Bootstrapping 

In statistics refers to drawing conclusions about a statistics’ sampling distribution by resampling the sample with replacement data as though it were a population with a fixed size 

![Bootstrapping](figs/bootstrap.png){#fig-bootstrap width="600" height="300"}

## Comparison of Classifiers 




::: custom-small


|                | LCI    | Score   | UCI    |        |                | LCI    | Score   | UCI    |
|----------------|--------|--------|--------|--------|----------------|--------|--------|--------|
| **Basque**     |        |        |        |        | **Spanish**    |        |        |        |
| **WordUp.01**  | 0.5031 | 0.5716 | 0.6401 |        | **WordUp.02**  | 0.7734 | 0.8086 | 0.8437 |
| WordUp.02      | 0.4751 | 0.5444 | 0.6138 |        | WordUp.01      | 0.7537 | 0.7899 | 0.8261 |
| **MultiAztertest.01** | 0.4287 | 0.5007 | 0.5726 |        | **MultiAztertest.01** | 0.6987 | 0.7400 | 0.7814 |
| SQYQP.01       | 0.3497 | 0.4237 | 0.4976 |        | SQYQP.01       | 0.6310 | 0.6730 | 0.7149 |
| **MultiAztertest.02** | 0.2664 | 0.3402 | 0.4139 |        | **MultiAztertest.02** | 0.5945 | 0.6391 | 0.6837 |

:  Results Table for Basque and Spanish {#tbl-independent .striped .hover}

:::




![Independent Samples](figs/ordenado2.png){#fig-independent-samples width="600" height="300"}

---

##  Difference against winner

::: custom-small


|                | LCI    | Diff   | UCI    |        |                | LCI    | Diff   | UCI    |
|----------------|--------|--------|--------|--------|----------------|--------|--------|--------|
| **Basque (WordUp.01)** |        |        |        |        | **Spanish (WordUp.02)** |        |        |        |
| **WordUp.02**  | -0.0371| 0.0269 | 0.0910 |        | **WordUp.01**  | -0.0120| 0.0184 | 0.0488 |
| MultiAztertest.01 | -0.0152| 0.0713 | 0.1578 |        | MultiAztertest.01 | 0.0211 | 0.0680 | 0.1149 |
| **SQYQP.01**   | 0.0543 | 0.1485 | 0.2427 |        | **SQYQP.01**   | 0.0877 | 0.1351 | 0.1825 |
| MultiAztertest.02 | 0.1405 | 0.2314 | 0.3222 |        | MultiAztertest.02 | 0.1165 | 0.1687 | 0.2210 |

:  Results Table for Basque (WordUp.01) and Spanish (WordUp.02) {#tbl-Paired .striped .hover}

![Paired Samples](figs/mejorordenado2.png){#fig-paired-samples width="600" height="300"}

:::

## Hypothesis Testing 



::: columns

::: {.column width="60%" }


Given the test dataset $x=x_1, \ldots , x_n$, in which $A$ beats $B$ by a magnitude $\delta(x)=\theta_A (x) - \theta_B (x)>0$ $$H_0:\theta_A \leq \theta_B;\;\;H_1:\theta_A > \theta_B$$

Estimate: $$p-value(x)=p(\delta(X)>\delta(x)|H_0,x)$$


As shown in Berg-Kirkpatrick (2012) \cite{Berg-Kirkpatrick2012}, the $p$-value$(x)$ can be estimated by computing the fraction of times that this difference is greater than $2\delta(x)$.

:::
::: {.column width="40%" .fragment}
![histogram of differences ](figs/histogram2.png){#fig-histogram .border .border-thick}
:::


:::

:::: {.fragment}

::: custom-small

|                     | WordUp.01  | WordUp.02  | MultiAztertest.01 | SQYQP.01  |
|---------------------|------------|------------|-------------------|-----------|
| **WordUp.02**        | 0.027      |            |                   |           |
| MultiAztertest.01    | 0.071 †    | 0.044      |                   |           |
| **SQYQP.01**         | 0.148 ***  | 0.121 **   | 0.077 *           |           |
| MultiAztertest.02    | 0.231 ***  | 0.204 ***  | 0.160 ***         | 0.083 *   |


:  Differences of F₁ score for Basque {#tbl-Basque .striped .hover}

:::


:::: footer
*Note: † p<.1, * p<.05, ** p<.1, and *** p<.001.*
::::

::::

## Multiple Testing 

### Risk of Multiple Testing


Multiple hypothesis testing increases the risk of Type I errors—the false rejection of a true null hypothesis.


### Correction Methods 


- **Bonferroni correction**: Divides the significance level by the number of comparisons.
- **Holm's step-down procedure**: Adjusts p-values sequentially.
- **Benjamini-Hochberg (BH) procedure**: Controls the false discovery rate (FDR).

---

## 

**Estimated p-values for F1 difference **



::: custom-small


|                   |                   | Difference | $p-value$  | Bonferroni | Holm     | BH       |
|-------------------|-------------------|------------|----------|------------|----------|----------|
| WordUp.01         | WordUp.02          | 0.027      | **0.2030** | **0.8120** | **0.2030** | **0.2030** |
| WordUp.01         | MultiAztertest.01  | 0.071      | **0.0551** | **0.2204** | **0.1102** | **0.0735** |
| WordUp.01         | SQYQP.01           | 0.148      | 0.0012   | 0.0048     | 0.0036   | 0.0024   |
| WordUp.01         | MultiAztertest.02  | 0.231      | 0.0000   | 0.0000     | 0.0000   | 0.0000   |
| WordUp.02         | MultiAztertest.01  | 0.044      | **0.1490** | **0.4470** | **0.1490** | **0.1490** |
| WordUp.02         | SQYQP.01           | 0.121      | 0.0039   | 0.0117     | 0.0078   | 0.0058   |
| WordUp.02         | MultiAztertest.02  | 0.204      | 0.0000   | 0.0000     | 0.0000   | 0.0000   |
| MultiAztertest.01 | SQYQP.01           | 0.077      | 0.0330   | **0.0660** | 0.0330   | 0.0330   |
| MultiAztertest.01 | MultiAztertest.02  | 0.160      | 0.0003   | 0.0006     | 0.0006   | 0.0006   |
| SQYQP.01          | MultiAztertest.02  | 0.083      | 0.0427   | 0.0427     | 0.0427   | 0.0427   |

: Estimated p-value for the F₁ difference without adjustment and with Bonferroni, FDR, Holm, and BH adjustments. {#tbl-p-values  .striped .hover}

:::

# Analysed Competitions

## Analysis of NLP Competitions  {.smaller .scrollable transition="slide"}


::: custom-small

| Competition        | Subtask / Language                                       | Metric Used                              | Data Considered                          |
|--------------------|----------------------------------------------------------|------------------------------------------|------------------------------------------|
| MEX-A3T 2019       | Author Profiling (Spanish, text and images)               | Macro-averaged F1 Score                  | All participants                         |
|                    | Aggressiveness Detection (Spanish)                       | F1 Score                                 | All participants                         |
| TASS 2020          | General Polarity (Spanish)                               | Macro-averaged F1 Score                  | All participants (Best Runs)             |
| **VaxxStance 2021**| **Stance Detection (Basque, Spanish)**                   | **Macro-averaged F1 Score for "favor" and "against"** | **All participants** |
| EXIST 2021         | Sexism Identification (English, Spanish)                 | Accuracy                                 | Top 10 for each language (Best Runs)     |
|                    | Sexism Categorization (English, Spanish)                 | Macro-averaged F1 Score                  | Top 10 for each language (Best Runs)     |
| DETOXIS 2021       | Toxicity Detection (Spanish)                             | F1 Score                                 | All participants (Best Runs)             |
| MeOffendEs 2021    | Offensive Language Identification (Mexican Spanish)      | F1 Score (offensive class)               | All participants (Best Runs)             |
| REST-MEX 2021      | Recommendation System (Mexican Spanish)                  | MAE                                      | All participants (baseline)              |
|                    | Sentiment Analysis (Mexican Spanish)                     | MAE                                      | All participants (baseline)              |
| REST-MEX 2022      | Sentiment Analysis (Mexican Spanish)                     | $Measure_S$                                | All participants (majority class)        |
|                    | Epidemiological Semaphore (Mexican Spanish)              | $Measure_C$                                | All participants (majority class)        |
| PAR-MEX 2022       | Paraphrase Identification (Mexican Spanish)              | F1 Score                                 | All participants (Best Runs)             |

:  Table Metrics and data considered across various competitions {#tbl-Competitions .striped .hover}

:::

---


## Results for the Various Challenges {.smaller .scrollable transition="slide"}


::: custom-small

| Challenge          | DETOXIS 2021 | PAR-MEX 2022 | MeOffendEs 2021 | MEX-A3T 2019 (Agg) | MEX-A3T 2019 (author profiling) |
|--------------------|--------------|--------------|-----------------|-------------------|--------------------------------|
| **Task**           | Toxicity detection | Paraphrase Identification | Non-contextual | Aggressiveness Detection | Author Profiling |
| **Metric**         | F1 score     | F1 score     | F1 score        | F1 score          | Macro-averaged F1 score       |
| **$n$**            | 891          | 2821         | 2182            | 3156              | 1500                          |
| **$m$**            | 31           | 8            | 10              | 25                | 4                             |
| **Ties w/ win.**   | 0/3/0/0      | 1/1/1/1      | 1/2/2/1         | 3/7/4/3           | 1/1/1/1                       |
| **Poss. compars.** | 465          | 28           | 45              | 300               | 6                             |
| **none/Bonf.**     | 80/135       | 6/6          | 7/9             | 70/91             | 2/2                           |
| **Holm/BH**        | 112/85       | 6/6          | 8/7             | 80/63             | 2/2                           |
| **$|win.-med|$**   | 0.223        | 0.061        | 0.078           | 0.098             | 0.164                         |
| **$CV$**           | 42.600       | 4.722        | 16.070          | 19.620            | 46.491                        |
| **PPI**            | 35.390       | 5.758        | 28.46           | 52.038            | 42.581                        |

:  Results for the Various Challenges {#tbl-Results .striped .hover}

:::


## Some Examples { .scrollable transition="slide"}

::: panel-tabset
### MeOffendEs subtask3

```{python}
#| echo: false
#| label: fig-MeOffendEs
#| fig-cap: "Confidence intervals"
#| fig-alt: "Confidence intervals"
import pickle
from CompStats import plot_performance_multiple, plot_difference_multiple, difference_multiple
# Recuperar el objeto del archivo
with open("datos/MeOffendEs_subtask3.dat", "rb") as archivo:
    perf = pickle.load(archivo)

face_grid = plot_performance_multiple(perf)


```


```{python}
#| echo: false
#| label: fig-MeOffendEs2
#| fig-cap: "Confidence Intervals of Differences"
#| fig-alt: "Confidence Intervals of Differences"
diff = difference_multiple(perf) 
face_grid_diff = plot_difference_multiple(diff)

```



### DETOXIS 2021

```{python}
#| echo: false
#| label: fig-detoxis
#| fig-cap: "Confidence intervals"
#| fig-alt: "Confidence intervals"

import pickle
from CompStats import plot_performance_multiple, plot_difference_multiple, difference_multiple
# Recuperar el objeto del archivo
with open("datos/detoxis_subtask1.dat", "rb") as archivo:
    perf = pickle.load(archivo)

face_grid = plot_performance_multiple(perf)


```


```{python}
#| echo: false
#| label: fig-detoxis2
#| fig-cap: "Confidence intervals of differences"
#| fig-alt: "Confidence intervals of differences"

diff = difference_multiple(perf) 
face_grid_diff = plot_difference_multiple(diff)

```

###  PAR-MEX 2022

```{python}
#| echo: false
#| label: fig-parmex
#| fig-cap: "Confidence intervals"
#| fig-alt: "Confidence intervals"

import pickle
from CompStats import plot_performance_multiple, plot_difference_multiple, difference_multiple
# Recuperar el objeto del archivo
with open("datos/PARMEX_2022.dat", "rb") as archivo:
    perf = pickle.load(archivo)

face_grid = plot_performance_multiple(perf)


```

```{python}
#| echo: false
#| label: fig-parmex2
#| fig-cap: "Confidence intervals of differences"
#| fig-alt: "Confidence intervals of differences"

diff = difference_multiple(perf) 
face_grid_diff = plot_difference_multiple(diff)

```


###  BoW


![Confidence Intervals](figs/IC.png){#fig-BoW-IC width="300" height="300"}

![Confidence Intervals of Differences](figs/ICD.png){#fig-BoW-ICD width="300" height="300"}



:::


## CompStats {.smaller .scrollable transition="slide"}


The package _CompStats_ [(compstats.readthedocs.org)](http://compstats.readthedocs.org) implements the ideas presented in this contribution.

### Installation

```python
pip install CompStats
```
### libraries

Once CompStats is installed, one must load a few libraries.

```{python}
#| echo: true
#| label: lib-compstats
from CompStats import performance, difference, plot_performance, plot_difference, difference_p_value
from statsmodels.stats.multitest import multipletests
from sklearn.metrics import f1_score
import pandas as pd
```

### Data
Let us assume *PARMEX_2022.csv* is a csv file where the column $y$ has the ground truth, and the other columns are the systems'outputs.

```{python}
#| echo: true
#| label: read_data
DATA = 'PARMEX_2022.csv'
df = pd.read_csv(DATA)
```
The performance metric used is the F1 score.



### Metric
```{python}
#| echo: true
#| label: Metric
score = lambda y, hy: f1_score(y, hy)
```

### Performance
```{python}
#| echo: true
#| label: fig-parmex3
#| fig-cap: "Confidence intervals"
#| fig-alt: "Confidence intervals"

perf = performance(df, score=score)
ins = plot_performance(perf)
```



### Performance Differences Against the Winner
```{python}
#| echo: true
#| #| echo: false
#| label: fig-parmex4
#| fig-cap: "Confidence intervals of differences"
#| fig-alt: "Confidence intervals of differences"

diff = difference(perf)
ins = plot_difference(diff)
```



### Performance Differences Against the Winner
```{python}
#| echo: true
#| label: p-value_difference
p_values = difference_p_value(diff)
p_values
```

### Bonferroni Correction
```{python}
#| echo: true
#| label: p-value_difference_bonferroni
result = multipletests(list(p_values.values()), method='bonferroni')
p_valuesC = dict(zip(p_values.keys(), result[1]))
p_valuesC

```

## Summary of Research
- **Objectives**: 

To investigate and implement an evaluation scheme based on robust statistical methods for learning algorithms in the context of challenges, providing organizers and researchers with concrete and validated tools for more precise and well-founded decision-making.



1. **Thoroughly review the different elements used in evaluating learning algorithms**
2. **Examine in detail the characteristics and criteria used by the main Machine Learning competitions**
3. **Design evaluation schemes for learning algorithms that integrate advanced statistical tools** 

  
- **Key Findings**:
  - Non-parametric Statistics are suitable for evaluating classification algorithms.
  - Introduced tools for fair competition result comparison.
  - Developed a structured evaluation framework for competitions.

---

## Contributions to Knowledge
- **Theoretical**:
  - Formalization of competition-based algorithm evaluation.
  - Expanded understanding of statistical tests and competition comparison.

- **Practical**:
  - Toolkit for statistical evaluation.
  - Actionable feedback for participants in algorithmic competitions.

- **Limitations**:
  - Data availability and external validity.
  - Focus on classification competitions.
  
## Acknowledgments

We extend our sincere gratitude to the organizers of the competitions analyzed in this chapter. Their dedication and commitment to fostering innovation and excellence have created valuable platforms that inspire participants to achieve their highest potential. Without their efforts, this comparative analysis would not have been possible. Thank you for providing these opportunities and for promoting a spirit of healthy competition and creativity.




## Thank you for your attention!



Please feel free to ask any questions.
